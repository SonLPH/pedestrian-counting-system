{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark cofiguration\n",
    "spark = SparkSession.builder.master('local')\\\n",
    "                            .appName('spark')\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(schema, csvfile):\n",
    "    '''Takes csv file's name and schema, returns the dataframe'''\n",
    "    df = spark.read.option(\"header\", True)\\\n",
    "                .schema(schema)\\\n",
    "                .csv(csvfile, header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rank \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, IntegerType, StructType, StringType, StructField, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_schema = StructType([\\\n",
    "    StructField(\"sensor_id\",IntegerType(),True),\\\n",
    "    StructField(\"sensor_description\",StringType(),True),\\\n",
    "    StructField(\"sensor_name\",StringType(),True),\\\n",
    "    StructField(\"installation_date\",TimestampType(),True),\\\n",
    "    StructField(\"status\",StringType(),True),\\\n",
    "    StructField(\"note\",StringType(),True),\\\n",
    "    StructField(\"direction_1\",StringType(),True),\\\n",
    "    StructField(\"direction_2\",StringType(),True),\\\n",
    "    StructField(\"latitude\",IntegerType(),True),\\\n",
    "    StructField(\"longitude\",IntegerType(),True),\\\n",
    "    StructField(\"location\",StringType(),True)\\\n",
    "    ])\n",
    "pedestrian_counting_schema = StructType([\\\n",
    "    StructField(\"ID\",IntegerType(),True),\\\n",
    "    StructField(\"Date_Time\",StringType(),True),\\\n",
    "    StructField(\"Year\",IntegerType(),True),\\\n",
    "    StructField(\"Month\",StringType(),True),\\\n",
    "    StructField(\"Mdate\",IntegerType(),True),\\\n",
    "    StructField(\"Day\",StringType(),True),\\\n",
    "    StructField(\"Time\",IntegerType(),True),\\\n",
    "    StructField(\"Sensor_ID\",IntegerType(),True),\\\n",
    "    StructField(\"Sensor_Name\",StringType(),True),\\\n",
    "    StructField(\"Hourly_Counts\",IntegerType(),True),\\\n",
    "    ])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = extract_from_csv(sensor_schema,'./Pedestrian_Counting_System_-_Sensor_Locations.csv')\n",
    "pedestrian_counting = extract_from_csv(pedestrian_counting_schema,'./Pedestrian_Counting_System_-_Monthly__counts_per_hour_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- sensor_description: string (nullable = true)\n",
      " |-- sensor_name: string (nullable = true)\n",
      " |-- installation_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- direction_1: string (nullable = true)\n",
      " |-- direction_2: string (nullable = true)\n",
      " |-- latitude: integer (nullable = true)\n",
      " |-- longitude: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing data\n",
    "from pyspark.sql.functions import to_date, date_format, to_csv\n",
    "pedestrian_counting = pedestrian_counting.withColumn(\"Date_Time\", to_date(col('Date_Time'), 'MMMM dd, yyyy hh:mm:ss a'))\\\n",
    "                                    .withColumn('Month', date_format(col('Date_Time'), 'M'))\\\n",
    "                                    .withColumn('Month', col('Month').cast('int'))\\\n",
    "                                    .withColumn('Hourly_Counts', col('Hourly_Counts').cast('int'))\\\n",
    "                                    .drop('Date_Time', 'Day', 'Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP 10 (MOST PEDESTRIANS) LOCATIONS BY DAY ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(['Year','Month', 'Mdate'])\\\n",
    "                            .orderBy(col('sum(Hourly_Counts)').desc())\n",
    "\n",
    "top_by_day = pedestrian_counting.groupBy(['Year','Month','Mdate','Sensor_ID']).sum('Hourly_Counts')\\\n",
    "                    .select('*', rank().over(window).alias('rank'))\\\n",
    "                    .filter(col('rank') <= 10)\\\n",
    "                    .withColumnRenamed('sum(Hourly_Counts)', 'Daily_Counts')\\\n",
    "                    .drop('rank')\\\n",
    "                    .orderBy(col('Year').asc(), col('Month').asc(),col('Mdate').asc(), col('Daily_Counts').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP 10 (MOST PEDESTRIANS) LOCATIONS BY MONTH ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(['Year','Month'])\\\n",
    "                            .orderBy(col('sum(Hourly_Counts)').desc())\n",
    "\n",
    "top_by_month = pedestrian_counting.groupBy(['Year','Month','Sensor_ID']).sum('Hourly_Counts')\\\n",
    "                    .select('*', rank().over(window).alias('rank'))\\\n",
    "                    .filter(col('rank') <= 10)\\\n",
    "                    .withColumnRenamed('sum(Hourly_Counts)', 'Monthly_Counts')\\\n",
    "                    .drop('rank')\\\n",
    "                    .orderBy(col('Year').asc(), col('Month').asc(), col('Monthly_Counts').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOST DECLINE PAST 2 YEARS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "\n",
    "most_decline = pedestrian_counting[pedestrian_counting['Year'].isin([2020,2021])]\\\n",
    "                                            .groupBy(['Sensor_ID'])\\\n",
    "                                            .pivot('Year')\\\n",
    "                                            .sum('Hourly_Counts')\\\n",
    "                                            .orderBy(col('Sensor_ID').asc())\\\n",
    "                                            .withColumn('Coef', col('2020') - col('2021'))\\\n",
    "                                            .orderBy(col('Coef').desc())\\\n",
    "                                            .limit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOST GROWTH LOCATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_growth = pedestrian_counting[pedestrian_counting['Year'].isin([2020,2021])]\\\n",
    "                                            .groupBy(['Sensor_ID'])\\\n",
    "                                            .pivot('Year')\\\n",
    "                                            .sum('Hourly_Counts')\\\n",
    "                                            .orderBy(col('Sensor_ID').asc())\\\n",
    "                                            .withColumn('Coef', col('2021') - col('2020'))\\\n",
    "                                            .orderBy(col('Coef').desc())\\\n",
    "                                            .limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with pd.ExcelWriter('./results.xlsx', engine=\"xlsxwriter\") as writer:\n",
    "    top_by_day.toPandas().to_excel(writer, sheet_name=\"Top_By_Day\", index=False)\n",
    "    top_by_month.toPandas().to_excel(writer, sheet_name=\"Top_By_Month\", index=False)\n",
    "    most_decline.toPandas().to_excel(writer, sheet_name=\"Most_Decline_Location\", index=False)\n",
    "    most_growth.toPandas().to_excel(writer, sheet_name=\"Most_Growth_Location\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c85eed8793fd0f257ae409aa9ad8f9aa8fe8059b3e0e7a3a58f5fead9212d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
